# -*- coding: utf-8 -*-
"""è¨ˆç®—æ€ç¶­ æœŸæœ«å ±å‘Š æ¨¡å‹æ¸¬è©¦.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Pje-XPY-8YbPodPMuzeOJ2-YKP9JHqOx
"""

!pip install datasets
!pip install scikit-learn
!pip install -U transformers --upgrade --force-reinstall
!pip install pandas

from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from datasets import Dataset
import pandas as pd
import os
os.environ["WANDB_DISABLED"] = "true"

# è®€å–è³‡æ–™é›†
df = pd.read_csv("final_fraud_and_normal_messages_1000.csv")

# æ‹†åˆ†è¨“ç·´é›†èˆ‡æ¸¬è©¦é›†
train_df, test_df = train_test_split(df, test_size=0.2, stratify=df["æ˜¯å¦è©é¨™"])

# è½‰æ›ç‚º Hugging Face çš„ Dataset æ ¼å¼
train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))
test_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))


# è¼‰å…¥ BERT åˆ†è©å™¨
tokenizer = BertTokenizer.from_pretrained("bert-base-chinese")

def tokenize(example):
    return tokenizer(example["ç•™è¨€å…§å®¹"], padding="max_length", truncation=True)

train_dataset = train_dataset.map(tokenize, batched=True)
test_dataset = test_dataset.map(tokenize, batched=True)
train_dataset = train_dataset.rename_column("æ˜¯å¦è©é¨™", "label")
test_dataset = test_dataset.rename_column("æ˜¯å¦è©é¨™", "label")
# è¼‰å…¥ BERT æ¨¡å‹ï¼ˆåˆ†é¡ç”¨ï¼‰
model = BertForSequenceClassification.from_pretrained("bert-base-chinese", num_labels=2)

# è¨­å®šè¨“ç·´åƒæ•¸
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    # Changed evaluation_strategy to eval_strategy
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_dir="./logs",
    # Add remove_unused_columns=False to prevent issues with extra columns
    remove_unused_columns=False
)

from sklearn.metrics import accuracy_score

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = logits.argmax(axis=-1)
    acc = accuracy_score(labels, predictions)
    return {"accuracy": acc}

# å»ºç«‹ Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics,
)

# é–‹å§‹è¨“ç·´
trainer.train()
trainer.save_model("./results")

# å£“ç¸®æ•´å€‹æ¨¡å‹è³‡æ–™å¤¾
!zip -r bert_trained_model.zip ./results

# ä¸‹è¼‰ zip æª”
from google.colab import files
files.download("bert_trained_model.zip")

trainer.save_model("./results")

from transformers import BertTokenizer, BertForSequenceClassification
import torch

# è¼‰å…¥å‰›å‰›è¨“ç·´å¥½çš„æ¨¡å‹èˆ‡åˆ†è©å™¨
model = BertForSequenceClassification.from_pretrained("./results")
tokenizer = BertTokenizer.from_pretrained("bert-base-chinese")

# é æ¸¬å‡½å¼
def predict(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    with torch.no_grad():
        outputs = model(**inputs)
    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
    label = torch.argmax(probs).item()
    confidence = probs[0][label].item()
    return "è©é¨™ç•™è¨€" if label == 1 else "æ­£å¸¸ç•™è¨€", confidence

text = "å‡ºå”®iPad Miniï¼Œå…ˆåŒ¯æ¬¾ï¼Œé¢äº¤"
result, score = predict(text)
print(f"çµæœï¼š{result}ï¼ˆä¿¡å¿ƒåˆ†æ•¸ï¼š{score:.2f}ï¼‰")

text = "å‡ºå”®iPad Miniï¼Œå…ˆåŒ¯æ¬¾ä¿ç•™ï¼Œå¯æ–¼å°åŒ—é¢äº¤ï¼Œç§è¨ŠèŠ"
result, score = predict(text)
print(f"çµæœï¼š{result}ï¼ˆä¿¡å¿ƒåˆ†æ•¸ï¼š{score:.2f}ï¼‰")

text = "å‡ºå”®iPad Miniï¼Œå«é¢äº¤æ™‚ä»˜æ¬¾"
result, score = predict(text)
print(f"çµæœï¼š{result}ï¼ˆä¿¡å¿ƒåˆ†æ•¸ï¼š{score:.2f}ï¼‰")



# æº–ç¢ºåº¦
from sklearn.metrics import accuracy_score

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = logits.argmax(axis=-1)
    acc = accuracy_score(labels, predictions)
    return {"accuracy": acc}

trainer.evaluate()

results = trainer.evaluate()
print(f"æ¨¡å‹æº–ç¢ºç‡ï¼š{results['eval_accuracy']:.4f}")



"""â€» å¦‚ä½•åœ¨è‡ªå·±çš„é›»è…¦ä¸Šä½¿ç”¨ğŸ¤”ï¼š

"""

# Step 1ï¼šæ›è¼‰ Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Step 2ï¼šè¤‡è£½ zip æª”åˆ° Colab ä¸¦è§£å£“ç¸®
!cp "/content/drive/MyDrive/è¨ˆç®—æ€ç¶­èˆ‡äººå·¥æ™ºæ…§æœŸæœ«/bert_trained_model.zip" . #æ­¤è™•è¦æ”¹æˆzipåœ¨ä½ é›²ç«¯çš„ä¸Šå‚³ä½ç½®
!unzip bert_trained_model.zip -d ./results

# Step 3ï¼šå®‰è£å¥—ä»¶ï¼ˆåªéœ€åŸ·è¡Œä¸€æ¬¡ï¼‰
!pip install -U transformers
!pip install torch

# Step 4ï¼šè¼‰å…¥æ¨¡å‹ä¸¦å®šç¾©é æ¸¬å‡½å¼
from transformers import BertTokenizer, BertForSequenceClassification
import torch

# è¼‰å…¥æ¨¡å‹èˆ‡åˆ†è©å™¨
model = BertForSequenceClassification.from_pretrained("./results/results/checkpoint-300")
tokenizer = BertTokenizer.from_pretrained("bert-base-chinese")

# é æ¸¬å‡½å¼
def predict(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    with torch.no_grad():
        outputs = model(**inputs)
    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
    label = torch.argmax(probs).item()
    confidence = probs[0][label].item()
    return "è©é¨™ç•™è¨€" if label == 1 else "æ­£å¸¸ç•™è¨€", confidence

# Step 5ï¼šè¼¸å…¥ä½ è¦æ¸¬è©¦çš„ç•™è¨€
text = "å‡ºå”®PS5ä¸»æ©Ÿ"
result, score = predict(text)
print(f"åˆ¤æ–·çµæœï¼š{result}ï¼ˆä¿¡å¿ƒåˆ†æ•¸ï¼š{score:.2f}ï¼‰")